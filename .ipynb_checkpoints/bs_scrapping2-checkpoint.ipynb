{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BS scrapping test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://shinminyong.tistory.com/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, glob\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import urllib.request\n",
    "from selenium.webdriver import Chrome\n",
    "import json, re, sys, h5py\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import datetime as dt\n",
    "import pymysql\n",
    "import matplotlib.pyplot as plt\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pymysql.connect(host='localhost',port=3306, db='INVESTAR',user='root',passwd='tlqkfdk2',autocommit=True)\n",
    "cursor = connection.cursor()\n",
    "#comps = pd.read_sql(\"SELECT * FROM company_info\",connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2411"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(\"SELECT * FROM COMPANY_INFO;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_code = []\n",
    "ls_comp = []\n",
    "for e in range(len(ls)):\n",
    "    ls_code.append(ls[e][0])\n",
    "    ls_comp.append(ls[e][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 일단 모든 종목은 다 있네 네이버 금융에"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WTD\n",
    "\n",
    "1) all finance bs data\n",
    "2) + evitda\n",
    "3) 시가총액\n",
    "4) 수정종가는...? 어디랬지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WTD : 일단 성공!\n",
    "\n",
    "1) Annaul, Quarter 2개 구현\n",
    "\n",
    "2) h5로 파일 만들고 파일명으로 접근해서 loader를 만들기\n",
    "\n",
    "3) last_update_date로 update하는 방식 -> 주식가격 그 쪽도 그렇게 하자\n",
    "\n",
    "4) loader에서 YoY, QoQ도 구현\n",
    "\n",
    "5) 시간을 걸어야겠네 너무 빨라서 못 긁는 경우가 있어;;\n",
    "\n",
    "https://shinminyong.tistory.com/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSCrawler:\n",
    "    def __init__(self, argv):\n",
    "        \"\"\"Naver Finance : Financial Summary Crawler\"\"\"\n",
    "        self.driver_path = \"C:/Users/Bae Kyungmo/OneDrive/Desktop/WC_basic/chromedriver.exe\"\n",
    "        self.update_date = datetime.today().strftime('%Y-%m-%d')\n",
    "        self.conn = pymysql.connect(host='localhost',user='root',\n",
    "                                   password='tlqkfdk2',db='INVESTAR',charset='utf8')\n",
    "        with self.conn.cursor() as curs:\n",
    "            sql_load = \"\"\"\n",
    "            SELECT CODE, COMPANY FROM COMPANY_INFO\n",
    "            \"\"\"\n",
    "            curs.execute(sql_load)\n",
    "            comps_ls = curs.fetchall()\n",
    "            self.codes = [str(e[0]) for e in comps_ls]\n",
    "            self.comps = [str(e[1]) for e in comps_ls]\n",
    "            \n",
    "        self.conn.commit()\n",
    "        \n",
    "        self.url = 'https://finance.naver.com/item/coinfo.nhn?code={}&target=finsum_more'\n",
    "        \n",
    "    def __del__(self):\n",
    "        \"\"\"Disconnecting MariaDB\"\"\"\n",
    "        self.conn.close()\n",
    "        \n",
    "    def DataExistChecker(self, df, n=3):\n",
    "        \"\"\"At least three of items should be checked.\"\"\"\n",
    "        pointer=0\n",
    "        for i in range(n):\n",
    "            for idx, col in enumerate(df.columns):\n",
    "                if df.iloc[i,idx] == 0.0 or df.iloc[i,idx] == -1.0:\n",
    "                    pointer = idx\n",
    "                    break\n",
    "        return df.columns[pointer]\n",
    "\n",
    "    def EmptyChecker(self, df, cds):\n",
    "        if df.shape[0] == 0 or df.shape[1] == 0:\n",
    "            raise ValueError(\"There is no info in this dataframe!! : {}\".format(cds))\n",
    "    \n",
    "    def crawler(self):\n",
    "        browser = Chrome(self.driver_path)\n",
    "        t=[]\n",
    "        for idx, cds in enumerate(self.codes[:5]):\n",
    "            url = self.url.format(cds)\n",
    "            requests.get(url).raise_for_status()\n",
    "            browser.get(url)\n",
    "            browser.switch_to.frame(browser.find_element_by_id('coinfo_cp'))\n",
    "            print(\"Crwaling *Annual* Financial Summary of *{}*...\".format(self.comps[idx]+'-'+cds))\n",
    "            WebDriverWait(browser, 2).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"cns_Tab21\"]')))\n",
    "            browser.find_elements_by_xpath('//*[@id=\"cns_Tab21\"]')[0].click()\n",
    "            html = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "            html_Y = html.find('table',{'class':'gHead01 all-width','summary':'주요재무정보를 제공합니다.'}) \n",
    "            html_tmp = html_Y.find('thead').find_all('tr')[1].find_all('th',attrs={\"class\":re.compile(\"^r03c\")})\n",
    "            dates = [''.join(re.findall('[0-9/]',html_tmp[i].text)).replace('/','-') for i in range(len(html_tmp))]\n",
    "            html_tmp = html_Y.find('tbody').find_all('tr')\n",
    "            cols = []\n",
    "            for i in range(len(html_tmp)):\n",
    "                if '\\xa0' in html_tmp[i].find('th').text:\n",
    "                    item = re.sub('\\xa0','',html_tmp[i].find('th').text)\n",
    "                else:\n",
    "                    item = html_tmp[i].find('th').text\n",
    "                cols.append(item)\n",
    "            values = []\n",
    "            for i in range(len(html_tmp)):\n",
    "                tmp = html_tmp[i].find_all('td')\n",
    "                value_tmp = []\n",
    "                for j in range(len(tmp)):\n",
    "                    try :\n",
    "                        if tmp[j].text == '':\n",
    "                            value_tmp.append(0.0)\n",
    "                        else:\n",
    "                            value_tmp.append(float(tmp[j].text.replace(',','')))\n",
    "                    except :\n",
    "                        value_tmp.append(-1.0)\n",
    "\n",
    "                values.append(value_tmp)\n",
    "            df = pd.DataFrame(data=values, columns=dates, index=cols)\n",
    "            EmptyChecker(df, cds)\n",
    "            last_info_date = DataExistChecker(df)\n",
    "            df.to_hdf(f'./FullCache/Annual/fs_annual_{cds}_{last_info_date}_{self.update_date}.h5',key=cds,mode='w')\n",
    "            \n",
    "            print(\"Crwaling *Quarter* Financial Summary of *{}*...\".format(self.comps[idx]+'-'+cds))\n",
    "            WebDriverWait(browser, 1).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"cns_Tab22\"]')))\n",
    "            browser.find_elements_by_xpath('//*[@id=\"cns_Tab22\"]')[0].click()\n",
    "            html = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "            html_Y = html.find('table',{'class':'gHead01 all-width','summary':'주요재무정보를 제공합니다.'}) \n",
    "            html_tmp = html_Y.find('thead').find_all('tr')[1].find_all('th',attrs={\"class\":re.compile(\"^r03c\")})\n",
    "            dates = [''.join(re.findall('[0-9/]',html_tmp[i].text)).replace('/','-') for i in range(len(html_tmp))]\n",
    "            html_tmp = html_Y.find('tbody').find_all('tr')\n",
    "            cols = []\n",
    "            for i in range(len(html_tmp)):\n",
    "                if '\\xa0' in html_tmp[i].find('th').text:\n",
    "                    item = re.sub('\\xa0','',html_tmp[i].find('th').text)\n",
    "                else:\n",
    "                    item = html_tmp[i].find('th').text\n",
    "                cols.append(item)\n",
    "            values = []\n",
    "            for i in range(len(html_tmp)):\n",
    "                tmp = html_tmp[i].find_all('td')\n",
    "                value_tmp = []\n",
    "                for j in range(len(tmp)):\n",
    "                    try :\n",
    "                        if tmp[j].text == '':\n",
    "                            value_tmp.append(0.0)\n",
    "                        else:\n",
    "                            value_tmp.append(float(tmp[j].text.replace(',','')))\n",
    "                    except :\n",
    "                        value_tmp.append(-1.0)\n",
    "\n",
    "                values.append(value_tmp)\n",
    "            df = pd.DataFrame(data=values, columns=dates, index=cols)\n",
    "            EmptyChecker(df, cds)\n",
    "            last_info_date = DataExistChecker(df)\n",
    "            df.to_hdf(f'./FullCache/Quarter/fs_quarter_{cds}_{last_info_date}_{self.update_date}.h5',key=cds,mode='w')\n",
    "            \n",
    "            #t.append(df)\n",
    "        return \"Cache generation is well done.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crwaling *Annual* Financial Summary of *동화약품-000020*...\n",
      "Crwaling *Quarter* Financial Summary of *동화약품-000020*...\n",
      "Crwaling *Annual* Financial Summary of *KR모터스-000040*...\n",
      "Crwaling *Quarter* Financial Summary of *KR모터스-000040*...\n",
      "Crwaling *Annual* Financial Summary of *경방-000050*...\n",
      "Crwaling *Quarter* Financial Summary of *경방-000050*...\n",
      "Crwaling *Annual* Financial Summary of *메리츠화재-000060*...\n",
      "Crwaling *Quarter* Financial Summary of *메리츠화재-000060*...\n",
      "Crwaling *Annual* Financial Summary of *삼양홀딩스-000070*...\n",
      "Crwaling *Quarter* Financial Summary of *삼양홀딩스-000070*...\n"
     ]
    }
   ],
   "source": [
    "bb = BSCrawler('full')\n",
    "rest = bb.crawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
